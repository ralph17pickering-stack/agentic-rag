# 19. Citations Pipeline (Backend)

⚠️ **Medium** — Multi-file changes, new migration, streaming event plumbing.

## Goal
Track which retrieved chunks/docs were used in an assistant response and persist them as `used_sources` on the messages table. Expose them in the SSE done event and MessageResponse model so the frontend can later display source citations.

---

## Tasks

### 1. Database migration: `used_sources` column

**File:** `supabase/migrations/017_used_sources_on_messages.sql`

```sql
ALTER TABLE messages ADD COLUMN used_sources JSONB DEFAULT NULL;
```

No RLS changes needed — the column is on `messages` which already has user-scoped RLS.

**Validation:** Run migration; confirm column exists via `\d messages` in psql.

---

### 2. Pydantic model update

**File:** `app/backapp/frontend/app/models/messages.py`

Add to `MessageResponse`:
```python
used_sources: Optional[list[dict[str, Any]]] = None
```

**Validation:** `mypy`/`pyright` passes; existing message serialization unaffected.

---

### 3. Capture raw chunks in LLM service

**File:** `app/backapp/frontend/app/services/llm.py`

In `_execute_tool`, when tool name is `retrieve_documents`:
- After calling `ctx.retrieve_fn()` and building the formatted string, also build a list of source dicts from the raw chunks.
- Yield/return the sources as a new `ToolEvent(tool_name="retrieve_documents", data={"sources": [...]})`.

Source dict format per chunk:
```python
{
    "chunk_id": chunk["id"],
    "document_id": chunk["document_id"],
    "doc_title": chunk.get("doc_title") or "Untitled",
    "chunk_index": chunk.get("chunk_index", 0),
    "content_preview": chunk["content"][:200],
    "score": chunk.get("rerank_score") or chunk.get("rrf_score") or chunk.get("similarity") or 0.0,
}
```

Return signature of `_execute_tool` currently: `tuple[str, list[ToolEvent]]`.
The new ToolEvent is added to the list — no signature change needed.

**Validation:** Add a test message with retrieve_documents → confirm ToolEvent with `tool_name="retrieve_documents"` is yielded.

---

### 4. Accumulate sources in chat router

**File:** `app/backapp/frontend/app/routers/chat.py`

In `event_generator`:
- Add `accumulated_sources: list[dict] = []` alongside `full_content` and `accumulated_web_results`.
- When processing ToolEvents: if `event.tool_name == "retrieve_documents"`, extend `accumulated_sources` with `event.data["sources"]`.
- Deduplicate by `chunk_id` before storing (keep highest score if duplicated across RAG-Fusion rounds).
- Store `used_sources=accumulated_sources or None` when upserting the assistant message (alongside `web_results`).
- Include `used_sources` in the done event via the full `MessageResponse`.

**Validation:** Send a RAG query → confirm `used_sources` is non-null in DB row; confirm done event message includes it.

---

### 5. Apply migration

Apply migration 017 to local Supabase:
```bash
docker exec -i supabase-db psql -U postgres -d postgres -f - < supabase/migrations/017_used_sources_on_messages.sql
```

**Validation:**
```bash
docker exec supabase-db psql -U postgres -d postgres -c "\d messages"
# used_sources column should appear
```

---

## Acceptance Criteria

1. `messages.used_sources` column exists and accepts JSONB.
2. After a RAG query, the assistant message row in DB has `used_sources` populated with chunk metadata.
3. `MessageResponse.used_sources` is returned in the SSE done event.
4. Non-RAG queries (no retrieve_documents tool call) leave `used_sources` as null.
5. No regression in existing SSE streaming or web results persistence.

---

## Dependencies / Gotchas

- `_execute_tool` currently returns `tuple[str, list[ToolEvent]]` — the retrieve_documents branch currently returns an empty list; just add the new event to it.
- RAG-Fusion runs multiple retrieval rounds — deduplicate by chunk_id in the router.
- The `used_sources` list could be large; truncate to top 20 by score if needed to keep DB rows manageable.
