# Phase 5: Pre-chunk Cleaning (Minimal) Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Implement minimal text cleaning steps before chunking in the RAG pipeline to improve quality of processed documents.

**Architecture:** Add a clean_text function that removes excessive whitespace, non-printable characters, and standardizes line breaks. This function will be called during document ingestion before chunking.

**Tech Stack:** Python, FastAPI, custom document processing pipeline.

---

### Task 1: Create document cleaner service

**Files:**
- Create: `app/backapp/frontend/app/services/document_cleaner.py`
- Create: `tests/unit/services/test_document_cleaner.py`

**Step 1: Write the failing test**

```python
import pytest
from app.services.document_cleaner import clean_text

def test_clean_text_removes_extra_whitespace():
    input_text = "  Hello   world  \n\n  test  "
    expected = "Hello world test"
    result = clean_text(input_text)
    assert result == expected
```

**Step 2: Run test to verify it fails**

Run: `pytest tests/unit/services/test_document_cleaner.py -k test_clean_text_removes_extra_whitespace -v`
Expected: FAILED (ModuleNotFoundError)

**Step 3: Write minimal implementation**

```python
import re

def clean_text(text: str) -> str:
    cleaned = re.sub(r'\s+', ' ', text).strip()
    return cleaned
```

**Step 4: Run test to verify it passes**

Run: `pytest tests/unit/services/test_document_cleaner.py -k test_clean_text_removes_extra_whitespace -v`
Expected: PASSED

**Step 5: Commit**

```bash
git add app/backapp/frontend/app/services/document_cleaner.py tests/unit/services/test_document_cleaner.py
git commit -m "feat: add minimal text cleaning for RAG pipeline"
```

### Task 2: Integrate cleaning into document processing

**Files:**
- Modify: `app/backapp/frontend/app/services/llm.py`
- Create: `tests/unit/services/test_llm.py` (if not exists)

**Step 1: Write test to verify process_document uses clean_text**

```python
import pytest
from app.services.llm import process_document
from app.services.document_cleaner import clean_text

def test_process_document_clean_text_before_chunking():
    input_text = "  Hello   world  \n\n  test  "
    chunks = process_document(input_text)
    for chunk in chunks:
        assert '  ' not in chunk
        assert '\n\n' not in chunk
```

**Step 2: Modify llm.py to call clean_text before chunking**

In `app/backapp/frontend/app/services/llm.py`, within `process_document` function:

```python
from app.services.document_cleaner import clean_text

def process_document(text: str) -> List[str]:
    cleaned_text = clean_text(text)
    # Existing chunking logic
    chunks = [cleaned_text[i:i+500] for i in range(0, len(cleaned_text), 500)]
    return chunks
```

**Step 3: Run test to verify it passes**

Run: `pytest tests/unit/services/test_llm.py -k test_process_document_clean_text_before_chunking -v`
Expected: PASSED

**Step 4: Commit**

```bash
git add app/backapp/frontend/app/services/llm.py tests/unit/services/test_llm.py
git commit -m "feat: integrate text cleaning into document processing"
```

Plan complete and saved to `.agent/plans/20.phase-5-pre-chunk-cleaning-minimal.md`. Two execution options:

1. Subagent-Driven (this session) - I dispatch fresh subagent per task, review between tasks, fast iteration

2. Parallel Session (separate) - Open new session with executing-plans, batch execution with checkpoints

Which approach?