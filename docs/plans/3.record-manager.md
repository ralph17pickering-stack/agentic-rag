# Module 3: Record Manager

**Complexity:** ⚠️ **Medium** — Well-scoped changes across 3 layers (DB, backend, frontend). Single-pass feasible.

## Context

Currently, uploading the same file twice creates duplicate documents and chunks. There's no content hashing, no change detection, and no way to "update" a document — you must manually delete and re-upload. Retrieval returns duplicate chunks when the same content was indexed multiple times.

The Record Manager adds SHA-256 content hashing at both document and chunk levels to enable deduplication and efficient replacement.

## Core Behavior

**`POST /api/documents` becomes:**
1. Read file, compute SHA-256 of content
2. Check for exact duplicate (`user_id` + `content_hash`) → return existing doc (HTTP 200, `is_duplicate: true`)
3. Check for same filename → delete old document (cascades chunks), insert new
4. Otherwise → normal insert + ingest

---

## Task 1: Database Migration

**File:** `supabase/migrations/003_record_manager.sql`

```sql
ALTER TABLE documents ADD COLUMN content_hash TEXT;
CREATE UNIQUE INDEX idx_documents_user_content_hash ON documents(user_id, content_hash) WHERE content_hash IS NOT NULL;
CREATE INDEX idx_documents_user_filename ON documents(user_id, filename);

ALTER TABLE chunks ADD COLUMN content_hash TEXT;
CREATE INDEX idx_chunks_document_content_hash ON chunks(document_id, content_hash) WHERE content_hash IS NOT NULL;
```

- `content_hash` nullable for backward compatibility with existing docs
- Partial unique index avoids NULL conflicts on legacy rows

**Verify:** `docker exec supabase-db psql -U postgres -d postgres -c "\d documents"` — confirm `content_hash` column

---

## Task 2: Hashing Utility

**File:** `app/backapp/frontend/app/services/hashing.py` (new)

- `sha256_hex(data: bytes) -> str` — for file content (document-level)
- `sha256_text(text: str) -> str` — for chunk content (chunk-level)

Uses `hashlib.sha256` (stdlib).

---

## Task 3: Update Chunker

**File:** `app/backapp/frontend/app/services/chunker.py`

- Add `content_hash: str` field to `Chunk` model
- Compute `sha256_text(content)` for each chunk in `chunk_text()`

---

## Task 4: Update Pydantic Response Model

**File:** `app/backapp/frontend/app/models/documents.py`

Add to `DocumentResponse`:
- `content_hash: str | None = None`
- `is_duplicate: bool = False` (response-only, not in DB)

---

## Task 5: Document Router Dedup Logic (core change)

**File:** `app/backapp/frontend/app/routers/documents.py`

`upload_document` endpoint:
1. Compute `sha256_hex(content)` after reading file
2. Query for existing doc with same `(user_id, content_hash)` → return 200 with `is_duplicate: True`
3. Query for same `(user_id, filename)` → delete old doc (storage + DB, chunks cascade)
4. Insert new document with `content_hash` field
5. Use `JSONResponse` to control status code (200 vs 201)

Handle unique constraint violation gracefully (concurrent upload race condition → return existing doc).

---

## Task 6: Update Ingestion Service

**File:** `app/backapp/frontend/app/services/ingestion.py`

- Add `content_hash: chunk.content_hash` to each chunk row during insert
- Add early check that document still exists before processing (handles race with replacement)

---

## Task 7: Update TypeScript Types

**File:** `app/frontend/src/types/index.ts`

Add to `Document` interface:
- `content_hash: string | null`
- `is_duplicate?: boolean`

---

## Task 8: Update Upload Hook

**File:** `app/frontend/src/hooks/useDocuments.ts`

- Return the document from `uploadDocument` so caller can check `is_duplicate`
- On replacement (same filename, different content): filter out old doc from local state

---

## Task 9: Duplicate Feedback in UI

**File:** `app/frontend/src/components/documents/DocumentsPanel.tsx`

- Add info state for transient messages
- Show blue info banner: "X is already uploaded with identical content. Skipped." on duplicate
- Update `onUpload` prop type to return `Document`

---

## Implementation Order

```
Task 1 (Migration)     ─┐
Task 2 (Hashing)       ─┤── parallel, no deps
Task 4 (Pydantic)      ─┤
Task 7 (TS types)      ─┘
         │
Task 3 (Chunker)       ── depends on Task 2
Task 5 (Router)        ── depends on Tasks 1, 2, 4
Task 6 (Ingestion)     ── depends on Tasks 1, 3
         │
Task 8 (Hook)          ── depends on Tasks 5, 7
Task 9 (UI)            ── depends on Task 8
```

---

## Critical Files

| File | Change |
|------|--------|
| `supabase/migrations/003_record_manager.sql` | New migration |
| `app/backapp/frontend/app/services/hashing.py` | New file |
| `app/backapp/frontend/app/services/chunker.py` | Add content_hash to Chunk |
| `app/backapp/frontend/app/models/documents.py` | Add content_hash, is_duplicate |
| `app/backapp/frontend/app/routers/documents.py` | Dedup logic in upload |
| `app/backapp/frontend/app/services/ingestion.py` | Store chunk hashes, existence check |
| `app/frontend/src/types/index.ts` | Add new fields |
| `app/frontend/src/hooks/useDocuments.ts` | Handle duplicate response |
| `app/frontend/src/components/documents/DocumentsPanel.tsx` | Info banner |

---

## Verification

1. Apply migration → confirm columns exist
2. Upload `test.txt` → 201, chunks have `content_hash` values
3. Upload `test.txt` again (identical) → 200, `is_duplicate: true`, blue info message, no new row
4. Edit `test.txt`, upload again → 201, old doc deleted, new doc ingested
5. `GET /api/documents` → no duplicates, `content_hash` populated
6. Existing documents (pre-migration, NULL hash) still work in retrieval
7. Frontend builds with no TS errors
